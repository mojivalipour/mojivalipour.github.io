---
---

@article{riasatian2021fine,
  title={Fine-tuning and training of densenet for histopathology image representation using tcga diagnostic slides},
  author={Riasatian, Abtin and Babaie, Morteza and Maleki, Danial and Kalra, Shivam and Valipour, Mojtaba and Hemati, Sobhan and Zaveri, Manit and Safarpoor, Amir and Shafiei, Sobhan and Afshari, Mehdi and others},
  journal={Medical Image Analysis},
  volume={70},
  pages={102032},
  year={2021},
  publisher={Elsevier},
  website={https://www.sciencedirect.com/science/article/pii/S1361841521000785},
  arxiv={2101.07903},
  code={https://github.com/RhazesLab/KimiaNet},
  preview={kimianet.jpeg},
  abstract={Feature vectors provided by pre-trained deep artificial neural networks have become a dominant source for image representation in recent literature. Their contribution to the performance of image analysis can be improved through fine-tuning. As an ultimate solution, one might even train a deep network from scratch with the domain-relevant images, a highly desirable option which is generally impeded in pathology by lack of labeled images and the computational expense. In this study, we propose a new network, namely KimiaNet, that employs the topology of the DenseNet with four dense blocks, fine-tuned and trained with histopathology images in different configurations. We used more than 240,000 image patches with  pixels acquired at 20magnification through our proposed “high-cellularity mosaic” approach to enable the usage of weak labels of 7126 whole slide images of formalin-fixed paraffin-embedded human pathology samples publicly available through The Cancer Genome Atlas (TCGA) repository. We tested KimiaNet using three public datasets, namely TCGA, endometrial cancer images, and colorectal cancer images by evaluating the performance of search and classification when corresponding features of different networks are used for image representation. As well, we designed and trained multiple convolutional batch-normalized ReLU (CBR) networks. The results show that KimiaNet provides superior results compared to the original DenseNet and smaller CBR networks when used as feature extractor to represent histopathology images.},
}

@article{rajabzadehqdylora,
  title={QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning},
  author={Rajabzadeh, Hossein and Valipour, Mojtaba and Tahaei, Marzieh and Kwon, Hyock Ju and Ghodsi, Ali and Chen, Boxing and Rezagholizadeh, Mehdi},
  year={2024},
  website={https://neurips2023-enlsp.github.io/papers/paper_84.pdf},
  preview={qdylora.png},
  abstract={Finetuning large language models requires huge GPU memory, restricting the choice to acquire Leger language models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring fine-tuning steps. This paper proposes QDyLoRA-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. QDyLoRA combines the advantages of QLoRA with Dynamic LoRA to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32GiG V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.},
}

@article{valipour2021symbolicgpt,
  title={Symbolicgpt: A generative transformer model for symbolic regression},
  author={Valipour, Mojtaba and You, Bowen and Panju, Maysum and Ghodsi, Ali},
  code={https://github.com/mojivalipour/symbolicgpt},
  data={https://huggingface.co/DataAnalyticsLab},
  preview={symbolicgpt.png},
  url={https://arxiv.org/abs/2106.14131},
  arxiv={2106.14131},
  journal={arXiv preprint arXiv:2106.14131},
  year={2021},
  selected={true},
  abstract={Symbolic regression is the task of identifying a mathematical expression that best fits a provided dataset of input and output values. Due to the richness of the space of mathematical expressions, symbolic regression is generally a challenging problem. While conventional approaches based on genetic evolution algorithms have been used for decades, deep learning-based methods are relatively new and an active research area. In this work, we present SymbolicGPT, a novel transformer-based language model for symbolic regression. This model exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility. Through comprehensive experiments, we show that our model performs strongly compared to competing models with respect to the accuracy, running time, and data efficiency.
  },
}

@inproceedings{valipour2023dylora,
  title={DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation},
  author={Valipour, Mojtaba and Rezagholizadeh, Mehdi and Kobyzev, Ivan and Ghodsi, Ali},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  publisher={EACL},
  pages={3266--3279},
  year={2023},
  selected={true},
  website={https://aclanthology.org/2023.eacl-main.239/},
  code={https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA},
  arxiv={2210.07558},
  preview={dylora.png},
  abstract={With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry. As a remedy, low-rank adapters (LoRA) keep the main pretrained weights of the model frozen and just introduce some learnable truncated SVD modules (so-called LoRA blocks) to the model. While LoRA blocks are parameter-efficient, they suffer from two major problems: first, the size of these blocks is fixed and cannot be modified after training (for example, if we need to change the rank of LoRA blocks, then we need to re-train them from scratch); second, optimizing their rank requires an exhaustive search and effort. In this work, we introduce a dynamic low-rank adaptation (DyLoRA) technique to address these two problems together. Our DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank by sorting the representation learned by the adapter module at different ranks during training. We evaluate our solution on different natural language understanding (GLUE benchmark) and language generation tasks (E2E, DART and WebNLG) using different pretrained models such as RoBERTa and GPT with different sizes. Our results show that we can train dynamic search-free models with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA without significantly compromising performance. Moreover, our models can perform consistently well on a much larger range of ranks compared to LoRA.
},
}

@inproceedings{asali2016using,
  title={Using Machine Learning approaches to detect opponent formation},
  author={Asali, Ehsan and Valipour, Mojtaba and Zare, Nader and Afshar, Ardavan and Katebzadeh, MohammadReza and Dastghaibyfard, GH},
  booktitle={2016 Artificial Intelligence and Robotics (IRANOPEN)},
  pages={140--144},
  year={2016},
  organization={IEEE}
}

@inproceedings{mahmoudi2015assessing,
  title={Assessing the role of AR-based content in improving learning performance considering Felder-Silverman learning style},
  author={Mahmoudi, Maryam Tayefeh and Badie, Kambiz and Valipour, Mojtaba},
  booktitle={2015 International Conference on Interactive Collaborative Learning (ICL)},
  pages={838--843},
  year={2015},
  organization={IEEE}
}

@article{valipour2023sortednet,
  title={Sortednet, a place for every network and every network in its place: Towards a generalized solution for training many-in-one neural networks},
  author={Valipour, Mojtaba and Rezagholizadeh, Mehdi and Rajabzadeh, Hossein and Tahaei, Marzieh and Chen, Boxing and Ghodsi, Ali},
  journal={arXiv preprint arXiv:2309.00255},
  year={2023},
  selected={true},
  preview={sortednet.png},
  arxiv={2309.00255},
}

@article{kavehzadeh2023sorted,
  title={Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)},
  author={Kavehzadeh, Parsa and Valipour, Mojtaba and Tahaei, Marzieh and Ghodsi, Ali and Chen, Boxing and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2309.08968},
  year={2023},
  selected={true},
  arxiv={2309.08968},
  preview={sortedllama.png},
}